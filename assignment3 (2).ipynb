{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce432219-b1f8-4b9b-9ae7-a5f197e46f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "    \n",
    "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is \n",
    "the total number of target classes.\n",
    "The matrix compares the actual target values with those predicted by the machine learning model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f87bf-46c1-42c3-a963-1f57485829b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "   \n",
    "Pair confusion matrix arising from two clusterings [1]. The pair confusion matrix computes a 2 by 2 similarity \n",
    "matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the\n",
    "same or into different clusters under the true and predicted clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dab5f3-9cb7-43f8-a69e-f4ddfd168195",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "   \n",
    "Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an \n",
    "application and measuring how much the application improves. It is an end-to-end evaluation where we can understand\n",
    "if a particular improvement in a component is really going to help the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4ffa0-8388-49bb-be12-4fcbf214131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4\n",
    "   \n",
    "In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth \n",
    "(reference text) whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on \n",
    "the performance of other NLP systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529a991-04e0-4bc0-bc6e-ab5c2b39c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 5\n",
    "    \n",
    "The confusion matrix is a tool used to evaluate the performance of a model and is visually represented as a table. \n",
    "It provides a deeper layer of insight to data practitioners on the model's performance, errors, and weaknesses. \n",
    "This allows for data practitioners to further analyze their model through fine-tuning.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046512d3-43d6-4dd5-baee-9d10db858e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 6\n",
    "    \n",
    "Use clustering metrics such as Silhouette Coefficient and Davies-Bouldin index to assess the quality and \n",
    "performance of clusters. Use Adjusted Rand Index and Mutual Information to measure the similarity between two\n",
    "clusters. \n",
    "Use Root Mean Square Error to measure the accuracy of the clustering.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587a011-7252-4332-9571-019f7c462ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 7\n",
    "   \n",
    "Relying solely on accuracy as an evaluation metric for classification tasks can be limiting for several reasons:\n",
    "\n",
    "1. **Class Imbalance**: In datasets where the classes are imbalanced, accuracy can be misleading. A classifier might achieve high accuracy by simply predicting the majority class for all instances, without properly capturing the minority class. This can lead to poor performance in terms of correctly identifying the minority class.\n",
    "\n",
    "2. **Cost Sensitivity**: In many real-world scenarios, misclassifying certain classes may have more severe consequences than others. Accuracy treats all misclassifications equally, which may not align with the actual costs associated with different types of errors.\n",
    "\n",
    "3. **Ambiguity in Misclassifications**: Accuracy does not provide insight into the types of errors made by the classifier. It treats all misclassifications equally, without distinguishing between different types of mistakes. Understanding the nature of misclassifications can be crucial for refining and improving the classifier.\n",
    "\n",
    "4. **Classifier Robustness**: Accuracy does not consider the robustness of the classifier to variations in the dataset or the generalization performance to unseen data. A classifier with high accuracy on the training data may not necessarily generalize well to new data.\n",
    "\n",
    "To address these limitations, one can consider using additional evaluation metrics alongside accuracy:\n",
    "\n",
    "1. **Precision and Recall**: Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positives. These metrics are particularly useful for imbalanced datasets as they provide insight into how well the classifier performs for each class.\n",
    "\n",
    "2. **F1 Score**: The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall and is useful when there is an uneven class distribution.\n",
    "\n",
    "3. **Confusion Matrix Analysis**: Analyzing the confusion matrix can provide deeper insights into the types of errors made by the classifier. This can help identify specific classes that are frequently misclassified and guide improvements in the model.\n",
    "\n",
    "4. **Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)**: ROC curves plot the true positive rate against the false positive rate at various threshold settings, providing a comprehensive overview of the classifier's performance across different thresholds. AUC summarizes the ROC curve into a single metric, which is particularly useful for binary classification tasks.\n",
    "\n",
    "By considering a combination of these metrics, one can gain a more comprehensive understanding of the classifier's performance and make more informed decisions regarding model selection and optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
